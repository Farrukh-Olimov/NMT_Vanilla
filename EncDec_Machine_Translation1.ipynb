{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras \n",
    "import re\n",
    "from collections import Counter\n",
    "from tensorflow.keras.layers import TimeDistributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "urlOriginalSentence='/home/farrukh/Work/Datasets/UN_ru-en/en-ru/UNv1.0.en-ru.en'\n",
    "urlTargetSentence='/home/farrukh/Work/Datasets/UN_ru-en/en-ru/UNv1.0.en-ru.ru'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadDataset(url,intInd,lastInd):\n",
    "    sentences=[]\n",
    "    with open(url,'r') as f:\n",
    "        sentences=f.read().split('\\n')[intInd:lastInd]\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "startInd=0\n",
    "lastInd=40000\n",
    "originalSentence=loadDataset(urlOriginalSentence,startInd,lastInd)\n",
    "targetSentence=loadDataset(urlTargetSentence,startInd,lastInd)\n",
    "\n",
    "originalEvalSentence=loadDataset(urlOriginalSentence,lastInd,lastInd+2000)\n",
    "targetEvalSentence=loadDataset(urlTargetSentence,lastInd,lastInd+2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/farrukh/Work/Datasets/UN_ru-en/en-ru/originalSentence','w') as f:\n",
    "    f.write(\"\\n\".join(originalSentence))\n",
    "with open('/home/farrukh/Work/Datasets/UN_ru-en/en-ru/originalEvalSentence','w') as f:\n",
    "    f.write(\"\\n\".join(originalEvalSentence))\n",
    "with open('/home/farrukh/Work/Datasets/UN_ru-en/en-ru/targetSentence','w') as f:\n",
    "    f.write(\"\\n\".join(targetSentence))\n",
    "with open('/home/farrukh/Work/Datasets/UN_ru-en/en-ru/targetEvalSentence','w') as f:\n",
    "    f.write(\"\\n\".join(targetEvalSentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "originalSentence=originalSentence[6:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "targetSentence=targetSentence[6:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size=12000\n",
    "max_length_original=50 \n",
    "max_length_target=50\n",
    "print(max_length_original)\n",
    "print(max_length_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanOriginalSentence(sentences,wordList=None, addHeaders=False,max_length=None):\n",
    "    cleanedSentences=[]\n",
    "    for sentence in sentences:\n",
    "        sentence=re.sub('[.,\\'\\\"?~!#@$%^&*()]+',\"\",sentence.lower())\n",
    "        sentence=re.sub(\"[ ]+\",\" \",sentence)\n",
    "        sentence=re.sub('[^a-zA-Z]+',\" \",sentence)\n",
    "        if wordList is not None:\n",
    "            temp=[]\n",
    "            for word in sentence.split(\" \"):\n",
    "                if word in wordList:\n",
    "                    temp.append(word)\n",
    "                else:\n",
    "                    temp.append('unk')\n",
    "            if len(temp)>max_length-2:\n",
    "                temp=temp[:max_length-2]\n",
    "            sentence=\" \".join(temp)\n",
    "        sentence=' '.join([w for w in sentence.split(' ') ])\n",
    "        if addHeaders==True:\n",
    "            sentence='<start> ' + sentence+ ' <end>'\n",
    "\n",
    "        cleanedSentences.append(sentence)\n",
    "    return cleanedSentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanTargetSentence(sentences,wordList=None,addHeaders=False,max_length=None):\n",
    "    cleanedSentences=[]\n",
    "    for sentence in sentences:\n",
    "        sentence=re.sub('[.,\\'\\\"?~!#@$%^&*()]+',\"\",sentence.lower())\n",
    "        sentence=re.sub(\"[ ]+\",\" \",sentence)\n",
    "        sentence=re.sub('[^а-яА-Я]+',\" \",sentence) \n",
    "        \n",
    "        if wordList is not None:\n",
    "            temp=[]\n",
    "            for word in sentence.split(\" \"):\n",
    "                if word in wordList:\n",
    "                    temp.append(word)\n",
    "                else:\n",
    "                    temp.append('unk')\n",
    "            if len(temp)>max_length-2:\n",
    "                temp=temp[:max_length-2]\n",
    "            sentence=\" \".join(temp)\n",
    "            \n",
    "        sentence=' '.join([w for w in sentence.split(\" \")])\n",
    "        if addHeaders==True:\n",
    "            sentence='<start> ' + sentence+ ' <end>'\n",
    "\n",
    "        cleanedSentences.append(sentence)\n",
    "    return cleanedSentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('commercialization', 1)\n",
      "12000\n",
      "12000\n"
     ]
    }
   ],
   "source": [
    "originalSentence=cleanOriginalSentence(originalSentence)\n",
    "counterOriginal=Counter(\" \".join(originalSentence).split(\" \")).most_common(vocabulary_size)\n",
    "print(counterOriginal[-1])\n",
    "counterOriginal={x:y for x,y in counterOriginal}\n",
    "print(len(counterOriginal))\n",
    "\n",
    "targetSentence=cleanTargetSentence(targetSentence)\n",
    "counterTarget=Counter(\" \".join(targetSentence).split(\" \")).most_common(vocabulary_size)\n",
    "counterTarget={x:y for x,y in counterTarget}\n",
    "print(len(counterTarget))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "originalSentence=cleanOriginalSentence(originalSentence,counterOriginal.keys(),True,max_length_original)\n",
    "originalEvalSentence=cleanOriginalSentence(originalEvalSentence,counterOriginal.keys(),True,max_length_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "targetSentence=cleanTargetSentence(targetSentence,counterTarget.keys(),True,max_length_target)\n",
    "targetEvalSentence=cleanTargetSentence(targetEvalSentence,counterTarget.keys(),True,max_length_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentences,max_length):\n",
    "    tokenizer=keras.preprocessing.text.Tokenizer(filters=\"\",)\n",
    "    tokenizer.fit_on_texts(sentences)\n",
    "    tensor=tokenizer.texts_to_sequences(sentences)\n",
    "    tensor=keras.preprocessing.sequence.pad_sequences(tensor,padding='post',maxlen=max_length)\n",
    "    return tensor,tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "original,originalTokenizer=tokenize(originalSentence,max_length_original)\n",
    "target,targetTokenizer=tokenize(targetSentence,max_length_target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "originalEval=keras.preprocessing.sequence.pad_sequences(originalTokenizer.texts_to_sequences(originalEvalSentence),maxlen=max_length_original,padding='post')\n",
    "\n",
    "targetEval=keras.preprocessing.sequence.pad_sequences(targetTokenizer.texts_to_sequences(targetEvalSentence),maxlen=max_length_target,padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11913"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(originalTokenizer.index_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12002"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(targetTokenizer.index_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=64\n",
    "steps_per_epoch=len(original)//batch_size\n",
    "embedding_dims=300\n",
    "units=300\n",
    "vocab_original_size=len(originalTokenizer.index_word)+1\n",
    "vocab_target_size=len(targetTokenizer.index_word)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(encoderInp,decoderInp,max_length,batch_size,vocab_size):\n",
    "    X1,X2,y=[],[],[]\n",
    "    n=0\n",
    "    while 1:\n",
    "        for enc,dec in zip(encoderInp,decoderInp):\n",
    "            n+=1\n",
    "            for i in range(1,len(dec)):\n",
    "                in_seq,out_seq=dec[:i],dec[i]\n",
    "                in_seq=keras.preprocessing.sequence.pad_sequences([in_seq],maxlen=max_length)[0]\n",
    "                out_seq=keras.utils.to_categorical([out_seq],num_classes=vocab_size)[0]\n",
    "                X1.append(enc[::-1])\n",
    "                X2.append(in_seq)\n",
    "                y.append(out_seq)\n",
    "            if n==batch_size:\n",
    "                n=0\n",
    "                yield ([np.array(X1),np.array(X2)],np.array(y))\n",
    "                X1,X2,y=[],[],[]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator=generator(original,target,max_length_target,batch_size,vocab_target_size)\n",
    "val_generator=generator(originalEval,targetEval,max_length_target,batch_size,vocab_target_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputEncoder=keras.Input((max_length_original,),name='encoderInput')\n",
    "\n",
    "embedEnc=keras.layers.Embedding(vocab_original_size,embedding_dims,name='Encoder_Embedding')\n",
    "gruEnc=keras.layers.GRU(units,return_sequences=True,return_state=True,dropout=0.25,recurrent_dropout=0.25,name='Encoder_GRU1')\n",
    "se1=embedEnc(inputEncoder)\n",
    "se2_out,se2_hidden=gruEnc(se1) \n",
    "'''\n",
    "se2_out shape= (batch_size,max_length,units)\n",
    "se2_hidden shape= (batch_size,units)\n",
    "'''\n",
    "\n",
    "inputDecoder=keras.Input((max_length_target,),name='decoderInput')\n",
    "embedDec=keras.layers.Embedding(vocab_target_size,embedding_dims,name='Decoder_Embedding')\n",
    "gruDec=keras.layers.GRU(units,dropout=0.25,recurrent_dropout=0.25,return_sequences=True,return_state=True,name='Decoder_GRU1') ## try with statefull\n",
    "denseDec=keras.layers.Dense(vocab_target_size,activation='softmax')\n",
    "sd1=embedDec(inputDecoder)\n",
    "sd2_out,sd2_hidden,=gruDec(sd1,initial_state=[se2_hidden])\n",
    "sd3=denseDec(sd2_out[:,-1])\n",
    "model=keras.Model([inputEncoder,inputDecoder],sd3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "encoder_model=keras.Model(inputEncoder,[se2_hidden])\n",
    "\n",
    "decoder_state_inp=keras.Input((units,))\n",
    "decoder_out,decoder_state_out,=gruDec(sd1,initial_state=[decoder_state_inp])\n",
    "eval_out=denseDec(decoder_out[:,-1])\n",
    "decoder_model=keras.Model([decoder_state_inp,inputDecoder],[eval_out,decoder_state_out])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['acc'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/farrukh/Work/Datasets/GLOVE/glove.6B.300d.txt') as f:\n",
    "    lines=f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_index={}\n",
    "for line in lines.split('\\n'):\n",
    "    values=line.split(\" \")\n",
    "    word=values[0]\n",
    "    values=np.asarray(values[1:],dtype='float64')\n",
    "    if word in originalTokenizer.word_index.keys():\n",
    "        embedding_index[word]=values\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix=np.zeros((vocab_original_size,embedding_dims),dtype='float64')\n",
    "\n",
    "for word,i in originalTokenizer.word_index.items():\n",
    "    vec=embedding_index.get(word,None)\n",
    "    if vec is not None:\n",
    "        embedding_matrix[i]=vec\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[2].set_weights([embedding_matrix])\n",
    "model.layers[2].trainable=False\n",
    "del(embedding_index)\n",
    "del(embedding_matrix)\n",
    "del(lines)\n",
    "del(counterOriginal)\n",
    "del(counterTarget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback=keras.callbacks.ModelCheckpoint('/home/farrukh/Work/Machine Translation/NMT_Vanilla/NMT_VanillaGRU.hdf5',save_weights_only=True,monitor='val_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.fit_generator(train_generator,steps_per_epoch=len(target)//batch_size,epochs=10,callbacks=[callback],verbose=True,validation_data=val_generator,\n",
    "                    validation_steps=len(targetEval),shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size=24\n",
    "train_generator=generator(original,target,max_length_target,batch_size,vocab_target_size)\n",
    "val_generator=generator(originalEval,targetEval,max_length_target,batch_size,vocab_target_size)\n",
    "model.fit_generator(train_generator,steps_per_epoch=len(target)//batch_size,epochs=10,callbacks=[callback],verbose=True,validation_data=val_generator,\n",
    "                    validation_steps=len(targetEval),shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targetTokenizer.index_word[0]='unk'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp1=(original[2222])[::-1].reshape(1,-1)\n",
    "inp2='<start>'\n",
    "pred=[targetTokenizer.word_index[inp2]]\n",
    "text=\"\"\n",
    "inp1=encoder_model.predict([inp1])\n",
    "for i in range(max_length_target):\n",
    "    seq=[w for w in pred]\n",
    "    seq=keras.preprocessing.sequence.pad_sequences([seq],maxlen=max_length_target)\n",
    "    y_hat,inp1=decoder_model.predict([inp1,seq])\n",
    "    y_hat=np.argmax(y_hat)\n",
    "    pred.append(y_hat)\n",
    "    text=text+ \" \" + targetTokenizer.index_word[y_hat]\n",
    "    if targetTokenizer.index_word[y_hat]=='<end>':\n",
    "        break\n",
    "    decoder_model.reset_states()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\" \".join(originalTokenizer.index_word[w] for w in original[2222] if w!=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\" \".join(targetTokenizer.index_word[w] for w in target[2222] if w!=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(targetEval[321])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
